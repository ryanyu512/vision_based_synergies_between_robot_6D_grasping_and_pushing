{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from env import Env\n",
    "from agent import Agent\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise environment\n",
    "min_x, max_x =  -0.110 - 0.175,   -0.110 + 0.175\n",
    "min_y, max_y =   0.535 - 0.175,    0.535 + 0.175\n",
    "min_z, max_z =               0,              0.4 \n",
    "\n",
    "workspace_lim = np.asarray([[min_x, max_x], \n",
    "                            [min_y, max_y],\n",
    "                            [min_z, max_z]])\n",
    "\n",
    "print(f\"workspace space: \\n{workspace_lim}\")\n",
    "\n",
    "obj_dir = 'objects/blocks/'\n",
    "N_obj   = 4\n",
    "\n",
    "env = Env(obj_dir, N_obj, workspace_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Environment Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset(reset_obj = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env, N_batch = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.is_debug = True\n",
    "delta_move = agent.env.push_guidance_generation(max_move = 0.05)\n",
    "\n",
    "# item_poses = agent.env.update_item_pose()\n",
    "# print(f'item_pose: {item_poses[item_ind][0:3]}')\n",
    "\n",
    "gripper_pos = np.array([-0.11122626281611381, 0.4855598136140757, 0.2684023847637833])\n",
    "for i in range(len(delta_move)):\n",
    "    gripper_pos += np.array(delta_move[i][0:3])\n",
    "    print(f'gripper_pos: {gripper_pos}, type: {delta_move[i][-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.interact_by_guidance(max_episode = 1, grasp_guidance = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(agent.encoder, input_size=(1, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get color\n",
    "color_img, depth_img = agent.env.get_rgbd_data()\n",
    "print(f'dmin: {np.min(depth_img[:])}, dmax: {np.max(depth_img[:])}')\n",
    "\n",
    "#preprocess data\n",
    "in_color_img, in_depth_img = agent.preprocess_input(color_img, depth_img)\n",
    "print(in_color_img.shape)\n",
    "print(in_depth_img.shape)\n",
    "\n",
    "#add the extra dimension in the 1st dimension\n",
    "in_color_img = in_color_img.unsqueeze(0)\n",
    "in_depth_img = in_depth_img.unsqueeze(0)\n",
    "print(in_color_img.shape)\n",
    "print(in_depth_img.shape)\n",
    "\n",
    "#feed into encoder\n",
    "with torch.no_grad():\n",
    "    latent_vector, reconstructed = agent.encoder(in_depth_img)\n",
    "\n",
    "print(f'dmin: {torch.min(in_depth_img)}, dmax: {torch.max(in_depth_img)}')\n",
    "print('latent vector shape: ', latent_vector.shape)\n",
    "print('reconstructed shape: ', reconstructed.shape)\n",
    "\n",
    "#show depth image\n",
    "plt.imshow(in_depth_img[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed into actor\n",
    "with torch.no_grad():\n",
    "    a, a_type, z, normal, a_type_probs = agent.actor.get_actions(latent_vector)\n",
    "    \n",
    "print(f\"action: {a}, action_type: {a_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed into actor\n",
    "with torch.no_grad():\n",
    "\n",
    "    #compute one hot vector\n",
    "    a_type_onehot = torch.nn.functional.one_hot(a_type.long(), num_classes = 3).float()\n",
    "\n",
    "    q1 = agent.critic1(state = latent_vector, action = a, action_type = a_type_onehot)\n",
    "    q2 = agent.critic2(state = latent_vector, action = a, action_type = a_type_onehot)\n",
    "\n",
    "    tq1 = agent.critic1_target(state = latent_vector, action = a, action_type = a_type_onehot)\n",
    "    tq2 = agent.critic2_target(state = latent_vector, action = a, action_type = a_type_onehot)\n",
    "\n",
    "print(f\"q1: {q1}, q2: {q2}, tq1: {tq1}, tq2: {tq2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing raw data and preprocess input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_img, depth_img = agent.env.get_rgbd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_color_img, in_depth_img = agent.preprocess_input(color_img, depth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_color_img.shape)\n",
    "print(in_depth_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(in_depth_img.permute((1,2,0)))\n",
    "ax[1].imshow(in_color_img.permute((1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.interact()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
