{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from env import Env\n",
    "from agent import Agent\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace space: \n",
      "[[-0.285  0.065]\n",
      " [ 0.36   0.71 ]\n",
      " [ 0.     0.4  ]]\n",
      "[SUCCESS] restart environment\n",
      "[setup_rgbd_cam] \n",
      " [[ 9.99999992e-01  2.89813737e-05  1.26784815e-04 -1.06169154e-01]\n",
      " [ 2.90796779e-05 -9.99999699e-01 -7.75429384e-04  5.35611757e-01]\n",
      " [ 1.26762304e-04  7.75433064e-04 -9.99999691e-01  3.42376167e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "[SUCCESS] setup rgbd camera\n",
      "[SUCCESS] load obj paths\n",
      "[SUCCESS] randomly choose objects\n",
      "[SUCCESS] randomly choose object colors\n",
      "object 0: shape_0, pose: [-0.20163370404246994, 0.4024629930458873, 0.15, 2.6698950202255682, 3.2443683141271906, 2.8032962264941492]\n",
      "[SUCCESS] add objects to simulation\n"
     ]
    }
   ],
   "source": [
    "#initialise environment\n",
    "min_x, max_x =  -0.110 - 0.175,   -0.110 + 0.175\n",
    "min_y, max_y =   0.535 - 0.175,    0.535 + 0.175\n",
    "min_z, max_z =               0,              0.4 \n",
    "\n",
    "workspace_lim = np.asarray([[min_x, max_x], \n",
    "                            [min_y, max_y],\n",
    "                            [min_z, max_z]])\n",
    "\n",
    "print(f\"workspace space: \\n{workspace_lim}\")\n",
    "\n",
    "obj_dir = 'objects/blocks/'\n",
    "N_obj   = 3\n",
    "\n",
    "env = Env(obj_dir, N_obj, workspace_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Environment Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] restart environment\n",
      "[setup_rgbd_cam] \n",
      " [[ 9.99999990e-01  3.01665057e-05  1.38725731e-04 -1.06171375e-01]\n",
      " [ 3.02703689e-05 -9.99999719e-01 -7.48753515e-04  5.35606429e-01]\n",
      " [ 1.38703105e-04  7.48757707e-04 -9.99999710e-01  3.42390860e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "[SUCCESS] setup rgbd camera\n",
      "[SUCCESS] load obj paths\n",
      "object 0: shape_0, pose: [-0.1997834691444808, 0.38619137474456106, 0.026316947673774314, 0.014095854728885832, -0.004121689057108273, -0.13506204367169508]\n",
      "[SUCCESS] add objects to simulation\n"
     ]
    }
   ],
   "source": [
    "env.reset(reset_obj = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] initialise environment\n",
      "[SUCCESS] initialise networks\n",
      "[SUCCESS] initialise memory buffer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905969824/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env, N_batch = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[return_home] [-0.0006897854820629551, -0.00011562923627691966, 3.7950135190808595e-05] [0, 0, 0]\n",
      "[SUCCESS] return home pose\n"
     ]
    }
   ],
   "source": [
    "agent.is_debug = True\n",
    "delta_move = agent.env.push_guidance_generation(max_move = 0.05)\n",
    "\n",
    "gripper_pos = np.array([-0.11122626281611381, 0.4855598136140757, 0.2684023847637833])\n",
    "for i in range(len(delta_move)):\n",
    "    gripper_pos += np.array(delta_move[i][0:3])\n",
    "    print(f'gripper_pos: {gripper_pos}, type: {delta_move[i][-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== episode: 0 step: 0 ====\n",
      "[return_home] [-0.0004854973945755558, -2.2243481043042263e-05, -2.0140213320075496e-06] [0, 0, 0]\n",
      "[SUCCESS] return home pose\n",
      "[move_reward] r: -0.23462995139781778\n",
      "[grasp_reward] r: 0.0\n",
      "[executable_reward] r: 0\n",
      "[collision_to_ground_reward] r: 0\n",
      "[workingspace_reward] r: 0\n",
      "[STEP]: 0 [ACTION TYPE]: tensor([0.]) [REWARD]: -0.23462995139781778\n",
      "[MOVE]: [-0.01473706 -0.01647011 -0.03544261 -0.13505219]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/github_repository/vision_based_synergies_between_robot_6D_grasping_and_pushing/env.py:435: RuntimeWarning: invalid value encountered in divide\n",
      "  unit_dir  = move_vector/move_norm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[move_reward] r: -0.19276165112180405\n",
      "[grasp_reward] r: 0.0\n",
      "[executable_reward] r: 0\n",
      "[collision_to_ground_reward] r: 0\n",
      "[workingspace_reward] r: 0\n",
      "[STEP]: 1 [ACTION TYPE]: tensor([0.]) [REWARD]: -0.19276165112180405\n",
      "[MOVE]: [-0.01473706 -0.01647011 -0.03544261  0.        ]\n",
      "[move_reward] r: -0.15104025227485282\n",
      "[grasp_reward] r: 0.0\n",
      "[executable_reward] r: 0\n",
      "[collision_to_ground_reward] r: 0\n",
      "[workingspace_reward] r: 0\n",
      "[STEP]: 2 [ACTION TYPE]: tensor([0.]) [REWARD]: -0.15104025227485282\n",
      "[MOVE]: [-0.01473706 -0.01647011 -0.03544261  0.        ]\n",
      "[move_reward] r: -0.10934294308068905\n",
      "[grasp_reward] r: 0.0\n",
      "[executable_reward] r: 0\n",
      "[collision_to_ground_reward] r: 0\n",
      "[workingspace_reward] r: 0\n",
      "[STEP]: 3 [ACTION TYPE]: tensor([0.]) [REWARD]: -0.10934294308068905\n",
      "[MOVE]: [-0.01473706 -0.01647011 -0.03544261  0.        ]\n",
      "[move_reward] r: -0.06802190590595557\n",
      "[grasp_reward] r: 0.0\n",
      "[executable_reward] r: 0\n",
      "[collision_to_ground_reward] r: 0\n",
      "[workingspace_reward] r: 0\n",
      "[STEP]: 4 [ACTION TYPE]: tensor([0.]) [REWARD]: -0.06802190590595557\n",
      "[MOVE]: [-0.01473706 -0.01647011 -0.03544261  0.        ]\n",
      "[move_reward] r: -0.02878271525929503\n",
      "[grasp_reward] r: 0.0\n",
      "[executable_reward] r: 0\n",
      "[collision_to_ground_reward] r: 0\n",
      "[workingspace_reward] r: 0\n",
      "[STEP]: 5 [ACTION TYPE]: tensor([0.]) [REWARD]: -0.02878271525929503\n",
      "[MOVE]: [-0.01473706 -0.01647011 -0.03544261  0.        ]\n",
      "[GRASP] grasp something\n",
      "[move_reward] r: -0.0012518212297797523\n",
      "[GRASP] change_z: 0.04792848008634134\n",
      "[GRASP] successful grasp\n",
      "[grasp_reward] r: 2.0\n",
      "[executable_reward] r: 0\n",
      "[collision_to_ground_reward] r: 0\n",
      "[workingspace_reward] r: 0\n",
      "[STEP]: 6 [ACTION TYPE]: tensor([0.]) [REWARD]: 1.9987481787702202\n",
      "[MOVE]: [ 0.    0.   -0.03  0.  ]\n",
      "[SUCCESS] restart environment\n",
      "[setup_rgbd_cam] \n",
      " [[ 9.99999990e-01  3.01665057e-05  1.38725731e-04 -1.06171375e-01]\n",
      " [ 3.02703689e-05 -9.99999719e-01 -7.48753515e-04  5.35606429e-01]\n",
      " [ 1.38703105e-04  7.48757707e-04 -9.99999710e-01  3.42390860e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "[SUCCESS] setup rgbd camera\n",
      "[SUCCESS] load obj paths\n",
      "[SUCCESS] randomly choose objects\n",
      "[SUCCESS] randomly choose object colors\n",
      "object 0: shape_0, pose: [0.038934192129405604, 0.4578007967067642, 0.15, 2.0484818490017602, 5.46710737413383, 5.255678455213498]\n",
      "[SUCCESS] add objects to simulation\n",
      "[SUCCESS] finish one episode\n"
     ]
    }
   ],
   "source": [
    "agent.interact_by_guidance(max_episode = 1, grasp_guidance = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(agent.encoder, input_size=(1, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get color\n",
    "color_img, depth_img = agent.env.get_rgbd_data()\n",
    "print(f'dmin: {np.min(depth_img[:])}, dmax: {np.max(depth_img[:])}')\n",
    "\n",
    "#preprocess data\n",
    "in_color_img, in_depth_img = agent.preprocess_input(color_img, depth_img)\n",
    "print(in_color_img.shape)\n",
    "print(in_depth_img.shape)\n",
    "\n",
    "#add the extra dimension in the 1st dimension\n",
    "in_color_img = in_color_img.unsqueeze(0)\n",
    "in_depth_img = in_depth_img.unsqueeze(0)\n",
    "print(in_color_img.shape)\n",
    "print(in_depth_img.shape)\n",
    "\n",
    "#feed into encoder\n",
    "with torch.no_grad():\n",
    "    latent_vector, reconstructed = agent.encoder(in_depth_img)\n",
    "\n",
    "print(f'dmin: {torch.min(in_depth_img)}, dmax: {torch.max(in_depth_img)}')\n",
    "print('latent vector shape: ', latent_vector.shape)\n",
    "print('reconstructed shape: ', reconstructed.shape)\n",
    "\n",
    "#show depth image\n",
    "plt.imshow(in_depth_img[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed into actor\n",
    "with torch.no_grad():\n",
    "    a, a_type, z, normal, a_type_probs = agent.actor.get_actions(latent_vector)\n",
    "    \n",
    "print(f\"action: {a}, action_type: {a_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed into actor\n",
    "with torch.no_grad():\n",
    "\n",
    "    #compute one hot vector\n",
    "    a_type_onehot = torch.nn.functional.one_hot(a_type.long(), num_classes = 3).float()\n",
    "\n",
    "    q1 = agent.critic1(state = latent_vector, action = a, action_type = a_type_onehot)\n",
    "    q2 = agent.critic2(state = latent_vector, action = a, action_type = a_type_onehot)\n",
    "\n",
    "    tq1 = agent.critic1_target(state = latent_vector, action = a, action_type = a_type_onehot)\n",
    "    tq2 = agent.critic2_target(state = latent_vector, action = a, action_type = a_type_onehot)\n",
    "\n",
    "print(f\"q1: {q1}, q2: {q2}, tq1: {tq1}, tq2: {tq2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing raw data and preprocess input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_img, depth_img = agent.env.get_rgbd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_color_img, in_depth_img = agent.preprocess_input(color_img, depth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_color_img.shape)\n",
    "print(in_depth_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(in_depth_img.permute((1,2,0)))\n",
    "ax[1].imshow(in_color_img.permute((1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.interact()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
