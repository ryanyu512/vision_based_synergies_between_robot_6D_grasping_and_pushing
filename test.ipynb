{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from env import Env\n",
    "from agent import Agent\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace space: \n",
      "[[-0.285  0.065]\n",
      " [ 0.36   0.71 ]\n",
      " [ 0.     0.4  ]]\n",
      "[SUCCESS] restart environment\n",
      "[setup_rgbd_cam] \n",
      " [[ 9.99999992e-01  2.89813737e-05  1.26784815e-04 -1.06169154e-01]\n",
      " [ 2.90796779e-05 -9.99999699e-01 -7.75429384e-04  5.35611757e-01]\n",
      " [ 1.26762304e-04  7.75433064e-04 -9.99999691e-01  3.42376167e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "[SUCCESS] setup rgbd camera\n",
      "[SUCCESS] load obj paths\n",
      "[SUCCESS] randomly choose objects\n",
      "[SUCCESS] randomly choose object colors\n",
      "object 0: shape_0, pose: [-0.13458401716013738, 0.5383885981107812, 0.15, 1.4699236738507695, 3.2385404117966625, 0.8766803191757007]\n",
      "object 1: shape_1, pose: [-0.1463967293907642, 0.6587041825104886, 0.15, 4.4174883030573895, 3.8604746730793336, 5.157519176943474]\n",
      "object 2: shape_2, pose: [-0.2463409058743293, 0.5088075607723527, 0.15, 3.5104077297252236, 2.114596282370982, 0.3670739209043347]\n",
      "[SUCCESS] add objects to simulation\n"
     ]
    }
   ],
   "source": [
    "#initialise environment\n",
    "min_x, max_x =  -0.110 - 0.175,   -0.110 + 0.175\n",
    "min_y, max_y =   0.535 - 0.175,    0.535 + 0.175\n",
    "min_z, max_z =               0,              0.4 \n",
    "\n",
    "workspace_lim = np.asarray([[min_x, max_x], \n",
    "                            [min_y, max_y],\n",
    "                            [min_z, max_z]])\n",
    "\n",
    "print(f\"workspace space: \\n{workspace_lim}\")\n",
    "\n",
    "obj_dir = 'objects/blocks/'\n",
    "N_obj   = 3\n",
    "\n",
    "env = Env(obj_dir, N_obj, workspace_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Environment Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] restart environment\n",
      "[setup_rgbd_cam] \n",
      " [[ 9.99999990e-01  3.01665057e-05  1.38725731e-04 -1.06171375e-01]\n",
      " [ 3.02703689e-05 -9.99999719e-01 -7.48753515e-04  5.35606429e-01]\n",
      " [ 1.38703105e-04  7.48757707e-04 -9.99999710e-01  3.42390860e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "[SUCCESS] setup rgbd camera\n",
      "[SUCCESS] load obj paths\n",
      "object 0: shape_0, pose: [-0.12271089653634884, 0.5810109787367578, 0.018970500570462914, -2.931622389679219, -0.7626821183150228, -2.842469342182401]\n",
      "object 1: shape_1, pose: [-0.15086120418465288, 0.6328634692767564, 0.015098536641870778, 2.720410231954403, -0.3019349380592663, 2.1567981351597045]\n",
      "object 2: shape_2, pose: [-0.22501116827563156, 0.498512616991673, 0.03458481677287044, 1.5750400880280504, 1.3624525601345234, 2.518891099015809]\n",
      "[SUCCESS] add objects to simulation\n"
     ]
    }
   ],
   "source": [
    "env.reset(reset_obj = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] initialise environment\n",
      "[SUCCESS] initialise networks\n",
      "[SUCCESS] initialise memory buffer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1716905969824/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env, N_batch = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[return_home] [-0.0006938476395508227, -0.00010905491541380616, 2.827573549383989e-05] [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "agent.is_debug = True\n",
    "delta_move = agent.env.push_guidance_generation(max_move = 0.05)\n",
    "\n",
    "gripper_pos = np.array([-0.11122626281611381, 0.4855598136140757, 0.2684023847637833])\n",
    "for i in range(len(delta_move)):\n",
    "    gripper_pos += np.array(delta_move[i][0:3])\n",
    "    print(f'gripper_pos: {gripper_pos}, type: {delta_move[i][-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.interact_by_guidance(max_episode = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(agent.encoder, input_size=(1, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get color\n",
    "color_img, depth_img = agent.env.get_rgbd_data()\n",
    "print(f'dmin: {np.min(depth_img[:])}, dmax: {np.max(depth_img[:])}')\n",
    "\n",
    "#preprocess data\n",
    "in_color_img, in_depth_img = agent.preprocess_input(color_img, depth_img)\n",
    "print(in_color_img.shape)\n",
    "print(in_depth_img.shape)\n",
    "\n",
    "#add the extra dimension in the 1st dimension\n",
    "in_color_img = in_color_img.unsqueeze(0)\n",
    "in_depth_img = in_depth_img.unsqueeze(0)\n",
    "print(in_color_img.shape)\n",
    "print(in_depth_img.shape)\n",
    "\n",
    "#feed into encoder\n",
    "with torch.no_grad():\n",
    "    latent_vector, reconstructed = agent.encoder(in_depth_img)\n",
    "\n",
    "print(f'dmin: {torch.min(in_depth_img)}, dmax: {torch.max(in_depth_img)}')\n",
    "print('latent vector shape: ', latent_vector.shape)\n",
    "print('reconstructed shape: ', reconstructed.shape)\n",
    "\n",
    "#show depth image\n",
    "plt.imshow(in_depth_img[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed into actor\n",
    "with torch.no_grad():\n",
    "    a, a_type, z, normal, a_type_probs = agent.actor.get_actions(latent_vector)\n",
    "    \n",
    "print(f\"action: {a}, action_type: {a_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed into actor\n",
    "with torch.no_grad():\n",
    "\n",
    "    #compute one hot vector\n",
    "    a_type_onehot = torch.nn.functional.one_hot(a_type.long(), num_classes = 3).float()\n",
    "\n",
    "    q1 = agent.critic1(state = latent_vector, action = a, action_type = a_type_onehot)\n",
    "    q2 = agent.critic2(state = latent_vector, action = a, action_type = a_type_onehot)\n",
    "\n",
    "    tq1 = agent.critic1_target(state = latent_vector, action = a, action_type = a_type_onehot)\n",
    "    tq2 = agent.critic2_target(state = latent_vector, action = a, action_type = a_type_onehot)\n",
    "\n",
    "print(f\"q1: {q1}, q2: {q2}, tq1: {tq1}, tq2: {tq2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing raw data and preprocess input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_img, depth_img = agent.env.get_rgbd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_color_img, in_depth_img = agent.preprocess_input(color_img, depth_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(in_color_img.shape)\n",
    "print(in_depth_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(in_depth_img.permute((1,2,0)))\n",
    "ax[1].imshow(in_color_img.permute((1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.interact()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
